# -*- coding: utf-8 -*-
"""Implementation_of_AlexNet

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bWgsu_HrqnxADP2LuPGX_ItNnx5Z-95M

# Implementation of AlexNet

<a href="https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">"ImageNet Classification with Deep Convolutional Neural Networks"</a>by Alex Krizhevsky, llya Sutskever and Geoffrey E. Hinton.</br>
</hr>
## In the paper we can read:
1. “The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer.”

2. “We applied this normalization after applying the ReLU nonlinearity in certain layers.”

3. “If we set s < z, we obtain overlapping pooling. This is what we use throughout our network, with s = 2 and z = 3.”

4. “The first convolutional layer filters the 224×224×3 input image with 96 kernels of size 11×11×3 with a stride of 4 pixels (this is the distance between the receptive field centers of neighboring neurons in a kernel map)."

5. "The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48.

6. "The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers. The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer.”

7. ”The fourth convolutional layer has 384 kernels of size 3 × 3 × 192 , and the fifth convolutional layer has 256 kernels of size 3 × 3 × 192. The fully-connected layers have 4096 neurons each.”

8. "We use dropout in the first two fully-connected layers [...]"

<img src="https://raw.githubusercontent.com/Machine-Learning-Tokyo/DL-workshop-series/master/Part%20I%20-%20Convolution%20Operations/images/AlexNet.png" />

## Network architecture
- The network consists of 5 Convolutional layers and 3 Fully Connected Layers ([4])

- Max Pooling is applied Between the layers:

- 1conv-2conv ([5])

- 2conv-3conv ([6])

- 5conv-1fc ([4])

- Before Max Pooling a normalization technique is applied. At the paper a normalization method named LRN (Local Response Normalization) was used. However, since LRN is not part of the standard tensorflow.keras library and it is not in the scope of this section to teach how to write custom layers, we will use another method instead. We chose to replace LRN with Batch Normalization for this example
"""

# imports 
from tensorflow.keras.layers import Input, Conv2D, \
BatchNormalization, MaxPool2D, Flatten, Dense, Dropout

# Tensors and Layers
"""The idea is to build a graph of tensors which are connected through layers. We will start with the input tensor which is created by an Input() layer.

In the Input() layer we have to define the shape of the input object (e.g. a numpy array).

"""

input = Input(shape=(224, 224, 3))

type(input)

# 1st block
x = Conv2D(
    filters=96,
    kernel_size=11,
    strides=4,
    padding='same',
    activation='relu'
)(input) # 1st convolutional layer

x = BatchNormalization()(x)
x = MaxPool2D(pool_size=3, strides=2)(x)

# 2nd-5th block
x = Conv2D(
    filters=256,
    kernel_size=5,
    padding='same',
    activation='relu'
)(x) # 2nd convolutional layer

x = BatchNormalization()(x)
x = MaxPool2D(pool_size=3, strides=2)(x)

x = Conv2D(
    filters=384,
    kernel_size=3,
    padding='same',
    activation='relu'
)(x) # 3rd convolutional layer

x = Conv2D(
    filters=384,
    kernel_size=3,
    padding='same',
    activation='relu'
)(x) # 4th convolutional layer

x = Conv2D(
    filters=256,
    kernel_size=3,
    padding='same',
    activation='relu'
)(x)  # 5th convolutional layer

x = BatchNormalization()(x)
x = MaxPool2D(pool_size=3, strides=2)(x)

# Dense Layers
# Before passing the output tensor of the last 
# Convolutional layer (13x13x256) to the first Dense() 
# layer we have to flatten it to a one-dimension tensor. 
# We do it by using the Flatten() layer.

"""
The fully-connected layers have 4096 neurons each 
"5th convolutional layer" Dropout with drop probability 0.5
is used after the first 2 fully connected layers 6th layers
"""
x = Flatten()(x)
x = Dense(units=4096, activation='relu')(x)
x = Dense(units=4096, activation='relu')(x)
x = Dropout(rate=0.5)(x)

# Output layer
"""
Since the model is to be used for classifiction tasks, the output of the model will be a Dense() layer with:

number of units equal to the number of classes in our task which are 1000 based on [ix]
softmax actication if we target to only one class per image
"""

output = Dense(units=1000, activation='softmax')(x)

# Model
from tensorflow.keras import Model
model = Model(inputs=input, outputs=output)
model.summary()



